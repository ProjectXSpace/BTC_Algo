{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import get_features_and_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM regression on returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector (First Sample): [4320.661111111117, 4313.296499999998, 4313.8142000000025, 4139.704399999998, 48.092587697298235, 4376.440449600143, 4250.152550399852, 17.075320058893993, -2.8431784957280275, 54.29219000000007, 4342.780000000001, 27.8199457847807, 24.468002066163535, 0.0, 42.85714285714286]\n",
      "Target (First Sample): tf.Tensor(-0.0027796306066146803, shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 13:38:36.029935: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2023-12-03 13:38:36.029961: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-03 13:38:36.029968: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-03 13:38:36.030226: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-03 13:38:36.030596: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/cd/xc6lw60d5711s9k_4hjp_dwc0000gn/T/ipykernel_2679/1260344951.py:86: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_train.iloc[47], y_train[47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BTC:USDT_timestamp\n",
       "2017-08-25 11:00:00   -0.016635\n",
       "2017-08-25 12:00:00   -0.015247\n",
       "2017-08-25 13:00:00   -0.011621\n",
       "2017-08-25 14:00:00   -0.007201\n",
       "2017-08-25 15:00:00   -0.027162\n",
       "                         ...   \n",
       "2023-08-13 01:00:00   -0.006273\n",
       "2023-08-13 02:00:00   -0.004290\n",
       "2023-08-13 03:00:00   -0.000634\n",
       "2023-08-13 04:00:00    0.000657\n",
       "2023-08-13 05:00:00    0.002130\n",
       "Name: BTC:USDT_target, Length: 49305, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = \"BTC/USDT\"\n",
    "feature_lags = []\n",
    "valid_split = 2000\n",
    "train_length = 48\n",
    "batch_size = 32\n",
    "seed = 44\n",
    "\n",
    "df = get_features_and_target(\n",
    "    symbol,\n",
    "    days_to_forecast=1,\n",
    "    feature_lags=feature_lags,\n",
    "    model_type=\"reg\",\n",
    ")\n",
    "\n",
    "symbol = symbol.replace(\"/\", \":\")\n",
    "\n",
    "X = df.drop(columns=f\"{symbol}_target\")\n",
    "\n",
    "y = df[f\"{symbol}_target\"].copy()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = (\n",
    "    X[:-valid_split],\n",
    "    X[-valid_split:],\n",
    "    y[:-valid_split],\n",
    "    y[-valid_split:],\n",
    ")\n",
    "\n",
    "train_df = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_train,\n",
    "    targets=y_train[(train_length - 1) :],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "valid_df = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_valid,\n",
    "    targets=y_valid[(train_length - 1) :],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Check\n",
    "# Initialize variables to hold the first batch's data\n",
    "# Get the first batch from the dataset\n",
    "for X_batch, y_batch in train_df.take(1):\n",
    "    # Extract the first sample from the batch\n",
    "    first_sample_features = X_batch[0][-1].numpy().tolist()\n",
    "    first_sample_target = y_batch[0]\n",
    "\n",
    "    # Print the feature vector and its corresponding target\n",
    "    print(\"Feature Vector (First Sample):\", first_sample_features)\n",
    "    print(\"Target (First Sample):\", first_sample_target)\n",
    "    break  # Exit after processing the first batch\n",
    "\n",
    "X_train.iloc[47], y_train[47]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lst_mregression\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  multiple                  31        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1d (Conv1D)             multiple                  1860      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  multiple                  21840     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  multiple                  21840     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  120       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1830      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47552 (185.75 KB)\n",
      "Trainable params: 47521 (185.63 KB)\n",
      "Non-trainable params: 31 (128.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class LSTMregression(tf.keras.Model):\n",
    "    def __init__(self, num_units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.norm_layer = tf.keras.layers.Normalization()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            filters=num_units * 2,\n",
    "            kernel_size=2,\n",
    "            strides=1,\n",
    "            padding=\"causal\",\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(num_units, return_sequences=True, stateful=False)\n",
    "        )\n",
    "        self.lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(num_units, return_sequences=True, stateful=False)\n",
    "        )\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=activation,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm_layer(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "# Assuming df and X_train are already defined\n",
    "num_features = len(df.columns) - 1\n",
    "model = LSTMregression(num_features=num_features)\n",
    "\n",
    "# Adapt normalization layer\n",
    "model.norm_layer.adapt(X_train)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\", \"RootMeanSquaredError\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, monitor=\"accuracy\", restore_best_weights=True\n",
    ")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\"TB_regression_logs\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_mae\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=1,  # Number of epochs with no improvement\n",
    "    min_lr=0.0001,  # Minimum learning rate\n",
    ")\n",
    "history = model.fit(\n",
    "    train_df,\n",
    "    validation_data=valid_df,\n",
    "    callbacks=[early_stopping, tensorboard, reduce_lr],\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_regression_model.keras\")  # Save regression the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing classificaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch X shape: tf.Tensor(\n",
      "[[[3.98791444e+03 3.99111850e+03 3.99332180e+03 ... 4.12691418e+01\n",
      "   9.28571429e+01 3.57142857e+01]\n",
      "  [3.98707444e+03 3.99132450e+03 3.99361620e+03 ... 4.48170716e+01\n",
      "   8.57142857e+01 2.85714286e+01]\n",
      "  [3.98757889e+03 3.99173250e+03 3.99387920e+03 ... 5.60380796e+01\n",
      "   7.85714286e+01 2.14285714e+01]\n",
      "  ...\n",
      "  [3.93456889e+03 3.94889050e+03 3.97370180e+03 ... 4.02517141e+01\n",
      "   2.85714286e+01 0.00000000e+00]\n",
      "  [3.93641000e+03 3.94567800e+03 3.97233600e+03 ... 2.73721019e+01\n",
      "   2.14285714e+01 0.00000000e+00]\n",
      "  [3.93529667e+03 3.94265750e+03 3.97118800e+03 ... 2.21275047e+01\n",
      "   1.42857143e+01 0.00000000e+00]]\n",
      "\n",
      " [[9.36813667e+03 9.34704100e+03 9.33306280e+03 ... 2.03941418e+01\n",
      "   0.00000000e+00 6.42857143e+01]\n",
      "  [9.36539000e+03 9.35106200e+03 9.33216940e+03 ... 2.80374919e+01\n",
      "   8.57142857e+01 5.71428571e+01]\n",
      "  [9.36295444e+03 9.35489250e+03 9.33143260e+03 ... 3.60592276e+01\n",
      "   1.00000000e+02 5.00000000e+01]\n",
      "  ...\n",
      "  [9.64090000e+03 9.61000900e+03 9.46923360e+03 ... 4.52995359e+01\n",
      "   7.14285714e+00 2.14285714e+01]\n",
      "  [9.63464222e+03 9.61678200e+03 9.47393220e+03 ... 5.31236859e+01\n",
      "   0.00000000e+00 1.42857143e+01]\n",
      "  [9.62810333e+03 9.62316850e+03 9.47843080e+03 ... 4.41959773e+01\n",
      "   0.00000000e+00 7.14285714e+00]]\n",
      "\n",
      " [[4.95311122e+04 4.99232205e+04 4.93623616e+04 ... 5.27388701e+01\n",
      "   8.57142857e+01 1.42857143e+01]\n",
      "  [4.95058144e+04 4.98846630e+04 4.93693254e+04 ... 6.77399411e+01\n",
      "   7.85714286e+01 7.14285714e+00]\n",
      "  [4.94706856e+04 4.98307345e+04 4.93670346e+04 ... 7.23624323e+01\n",
      "   1.00000000e+02 0.00000000e+00]\n",
      "  ...\n",
      "  [4.83783456e+04 4.81355100e+04 4.86515406e+04 ... 8.41741768e+01\n",
      "   2.85714286e+01 1.00000000e+02]\n",
      "  [4.85013178e+04 4.81689550e+04 4.86358936e+04 ... 7.18131528e+01\n",
      "   2.14285714e+01 9.28571429e+01]\n",
      "  [4.86468733e+04 4.82151470e+04 4.86327000e+04 ... 5.91179762e+01\n",
      "   1.42857143e+01 8.57142857e+01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.00855378e+04 2.07136090e+04 2.14236974e+04 ... 3.37621365e+01\n",
      "   6.42857143e+01 0.00000000e+00]\n",
      "  [2.00422044e+04 2.06278190e+04 2.13836694e+04 ... 3.58618755e+01\n",
      "   5.71428571e+01 0.00000000e+00]\n",
      "  [1.99974156e+04 2.05452080e+04 2.13423392e+04 ... 3.66782730e+01\n",
      "   5.00000000e+01 0.00000000e+00]\n",
      "  ...\n",
      "  [2.48659122e+04 2.45278245e+04 2.29605350e+04 ... 8.38563679e+01\n",
      "   0.00000000e+00 9.28571429e+01]\n",
      "  [2.50402778e+04 2.46085625e+04 2.30776196e+04 ... 7.84450408e+01\n",
      "   0.00000000e+00 8.57142857e+01]\n",
      "  [2.52202300e+04 2.46952175e+04 2.31939224e+04 ... 7.51268973e+01\n",
      "   4.28571429e+01 7.85714286e+01]]\n",
      "\n",
      " [[9.12965667e+03 9.03632450e+03 8.94675160e+03 ... 7.67083360e+01\n",
      "   7.14285714e+00 9.28571429e+01]\n",
      "  [9.15433222e+03 9.05337900e+03 8.95706000e+03 ... 6.69104696e+01\n",
      "   0.00000000e+00 8.57142857e+01]\n",
      "  [9.18187556e+03 9.07179850e+03 8.96593820e+03 ... 5.99806545e+01\n",
      "   0.00000000e+00 7.85714286e+01]\n",
      "  ...\n",
      "  [9.86965889e+03 9.85091300e+03 9.50650520e+03 ... 5.70739820e+01\n",
      "   7.85714286e+01 2.14285714e+01]\n",
      "  [9.86968444e+03 9.87012500e+03 9.52284840e+03 ... 6.93789618e+01\n",
      "   7.14285714e+01 1.42857143e+01]\n",
      "  [9.85839889e+03 9.88002950e+03 9.53488660e+03 ... 7.22549547e+01\n",
      "   6.42857143e+01 7.14285714e+00]]\n",
      "\n",
      " [[7.63941667e+03 7.54451850e+03 7.52536240e+03 ... 8.90368239e+01\n",
      "   7.14285714e+01 1.00000000e+02]\n",
      "  [7.66675444e+03 7.55449550e+03 7.53420940e+03 ... 8.53929778e+01\n",
      "   6.42857143e+01 9.28571429e+01]\n",
      "  [7.68616778e+03 7.57218600e+03 7.53950940e+03 ... 7.31472389e+01\n",
      "   5.71428571e+01 8.57142857e+01]\n",
      "  ...\n",
      "  [7.32888111e+03 7.39375900e+03 7.57700380e+03 ... 7.38946493e+01\n",
      "   7.14285714e+01 0.00000000e+00]\n",
      "  [7.33827778e+03 7.38958900e+03 7.57293720e+03 ... 7.43756803e+01\n",
      "   6.42857143e+01 0.00000000e+00]\n",
      "  [7.33842778e+03 7.37958000e+03 7.56420100e+03 ... 6.73555298e+01\n",
      "   5.71428571e+01 0.00000000e+00]]], shape=(32, 48, 15), dtype=float64)\n",
      "Train batch y shape: tf.Tensor([0 1 1 1 0 2 0 0 1 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0], shape=(32,), dtype=int64)\n",
      "X_shape = (32, 48, 15)\n",
      "y_shape = (51305,)\n"
     ]
    }
   ],
   "source": [
    "symbol = \"BTC/USDT\"\n",
    "feature_lags = []\n",
    "valid_split = 2000\n",
    "train_length = 48\n",
    "batch_size = 32\n",
    "seed = 44\n",
    "\n",
    "df_classification = get_features_and_target(\n",
    "    symbol,\n",
    "    days_to_forecast=1,\n",
    "    feature_lags=feature_lags,\n",
    "    model_type=\"class\",\n",
    ")\n",
    "\n",
    "symbol = symbol.replace(\"/\", \":\")\n",
    "\n",
    "X = df_classification.drop(columns=f\"{symbol}_target\")\n",
    "\n",
    "y = df_classification[f\"{symbol}_target\"].copy().map(lambda x: int(x))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = (\n",
    "    X[:-valid_split],\n",
    "    X[-valid_split:],\n",
    "    y[:-valid_split],\n",
    "    y[-valid_split:],\n",
    ")\n",
    "\n",
    "train_df_classification = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_train,\n",
    "    targets=y_train[train_length:],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shaffles the sequences, not within sequences\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "valid_df_classification = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_valid,\n",
    "    targets=y_valid[train_length:],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "for X_batch, y_batch in train_df_classification.take(1):\n",
    "    print(\"Train batch X shape:\", X_batch)\n",
    "    print(\"Train batch y shape:\", y_batch)  # starts from y.iloc[48]\n",
    "\n",
    "print(f\"X_shape = {X_batch.shape}\")\n",
    "print(f\"y_shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lst_mclassification_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_13 (Normaliz  multiple                  31        \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          multiple                  1860      \n",
      "                                                                 \n",
      " bidirectional_26 (Bidirect  multiple                  21840     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_27 (Bidirect  multiple                  21840     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " layer_normalization_13 (La  multiple                  120       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_26 (Dense)            multiple                  1830      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            multiple                  93        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47614 (186.00 KB)\n",
      "Trainable params: 47583 (185.87 KB)\n",
      "Non-trainable params: 31 (128.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "class LSTMclassification(tf.keras.Model):\n",
    "    def __init__(self, num_classes, num_units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.norm_layer = tf.keras.layers.Normalization()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            filters=num_units * 2,\n",
    "            kernel_size=2,\n",
    "            strides=1,\n",
    "            padding=\"causal\",\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(num_units, return_sequences=True, stateful=False)\n",
    "        )\n",
    "        self.lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(num_units, return_sequences=False, stateful=False)\n",
    "        )\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=activation,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.output_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.norm_layer(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "# Assuming df, X_train, and num_classes are already defined\n",
    "num_features = len(df.columns) - 1\n",
    "model = LSTMclassification(num_classes=3, num_units=30)\n",
    "\n",
    "# Adapt normalization layer\n",
    "model.norm_layer.adapt(X_train)\n",
    "\n",
    "model.build(input_shape=(batch_size, train_length, num_features))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tensorboard \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(\u001b[39m\"\u001b[39m\u001b[39mTB_classification_logs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m reduce_lr \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,  \u001b[39m# Reduce learning rate by half\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     patience\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,  \u001b[39m# Number of epochs with no improvement\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     min_lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,  \u001b[39m# Minimum learning rate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     train_df_classification,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalid_df_classification,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping, tensorboard, reduce_lr],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# class_weight=class_weights,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/cd/xc6lw60d5711s9k_4hjp_dwc0000gn/T/__autograph_generated_file3vsvidsq.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     \"balanced\", classes=np.unique(y_train), y=y_train\n",
    "# )\n",
    "\n",
    "class_weights = {0: 0.48, 1: 2.26, 2: 1.95}\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, monitor=\"accuracy\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\"TB_classification_logs\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"accuracy\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=1,  # Number of epochs with no improvement\n",
    "    min_lr=0.0001,  # Minimum learning rate\n",
    ")\n",
    "model.fit(\n",
    "    train_df_classification,\n",
    "    validation_data=valid_df_classification,\n",
    "    callbacks=[early_stopping, tensorboard, reduce_lr],\n",
    "    epochs=1,\n",
    "    class_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_classification_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
