{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import get_features_and_target\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM regression on returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector (First Sample): [4320.661111111117, 4313.296499999998, 4313.8142000000025, 4139.704399999998, 48.092587697298235, 4376.440449600143, 4250.152550399852, 17.075320058893993, -2.8431784957280275, 54.29219000000007, 4342.780000000001, 27.8199457847807, 24.468002066163535, 0.0, 42.85714285714286]\n",
      "Target (First Sample): tf.Tensor(0.026239868113751896, shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/xc6lw60d5711s9k_4hjp_dwc0000gn/T/ipykernel_8564/1950526221.py:86: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  X_train_adjusted.iloc[47], y_train_adjusted[47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BTC:USDT_sma_9                  4320.661111\n",
       " BTC:USDT_sma_20                 4313.296500\n",
       " BTC:USDT_sma_50                 4313.814200\n",
       " BTC:USDT_sma_200                4139.704400\n",
       " BTC:USDT_rsi                      48.092588\n",
       " BTC:USDT_bollinger_up           4376.440450\n",
       " BTC:USDT_bollinger_down         4250.152550\n",
       " BTC:USDT_adx                      17.075320\n",
       " BTC:USDT_macd_diff                -2.843178\n",
       " BTC:USDT_obv                      54.292190\n",
       " BTC:USDT_ichimoku_conversion    4342.780000\n",
       " BTC:USDT_stochastic_k             27.819946\n",
       " BTC:USDT_stochastic_d             24.468002\n",
       " BTC:USDT_aroon_up                  0.000000\n",
       " BTC:USDT_aroon_down               42.857143\n",
       " Name: 2017-08-27 10:00:00, dtype: float64,\n",
       " 0.026239868113751896)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = \"BTC/USDT\"\n",
    "feature_lags = []\n",
    "valid_split = 2000\n",
    "train_length = 48\n",
    "batch_size = 32\n",
    "seed = 44\n",
    "\n",
    "df = get_features_and_target(\n",
    "    symbol,\n",
    "    days_to_forecast=1,\n",
    "    feature_lags=feature_lags,\n",
    "    model_type=\"reg\",\n",
    ")\n",
    "\n",
    "\n",
    "symbol = symbol.replace(\"/\", \":\")\n",
    "\n",
    "X = df.drop(columns=f\"{symbol}_target\")\n",
    "\n",
    "y = df[f\"{symbol}_target\"].copy()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = (\n",
    "    X[:-valid_split],\n",
    "    X[-valid_split:],\n",
    "    y[:-valid_split],\n",
    "    y[-valid_split:],\n",
    ")\n",
    "\n",
    "\n",
    "#### For Stateful LSTM\n",
    "# Function to adjust dataset\n",
    "def adjust_dataset(X, y, train_length, batch_size):\n",
    "    total_sequences = len(X) - train_length + 1\n",
    "    complete_batches = total_sequences // batch_size\n",
    "    keep_sequences = complete_batches * batch_size\n",
    "    keep_data_points = keep_sequences + train_length - 18\n",
    "\n",
    "    X_adjusted = X.iloc[:keep_data_points]\n",
    "    y_adjusted = y.iloc[train_length - 1 : keep_data_points]\n",
    "\n",
    "    return X_adjusted, y_adjusted\n",
    "\n",
    "\n",
    "# Adjusting training dataset\n",
    "X_train_adjusted, y_train_adjusted = adjust_dataset(\n",
    "    X_train, y_train, train_length, batch_size\n",
    ")\n",
    "\n",
    "# Adjusting validation dataset\n",
    "X_valid_adjusted, y_valid_adjusted = adjust_dataset(\n",
    "    X_valid, y_valid, train_length, batch_size\n",
    ")\n",
    "\n",
    "# Now, create the datasets\n",
    "train_df_stateful = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_train_adjusted,\n",
    "    targets=y_train_adjusted[(train_length - 1) :],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "valid_df_stateful = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_valid_adjusted,\n",
    "    targets=y_valid_adjusted[(train_length - 1) :],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Check\n",
    "# Initialize variables to hold the first batch's data\n",
    "# Get the first batch from the dataset\n",
    "for X_batch, y_batch in train_df_stateful.take(1):\n",
    "    # Extract the first sample from the batch\n",
    "    first_sample_features = X_batch[0][-1].numpy().tolist()\n",
    "    first_sample_target = y_batch[0]\n",
    "\n",
    "    # Print the feature vector and its corresponding target\n",
    "    print(\"Feature Vector (First Sample):\", first_sample_features)\n",
    "    print(\"Target (First Sample):\", first_sample_target)\n",
    "    break  # Exit after processing the first batch\n",
    "\n",
    "X_train_adjusted.iloc[47], y_train_adjusted[47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(32, None, 15)]          0         \n",
      "                                                                 \n",
      " normalization (Normalizati  (32, None, 15)            31        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (32, None, 60)            2760      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (32, None, 60)            21840     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (32, 60)                  21840     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (32, 60)                  0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (32, 60)                  240       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (32, 30)                  1830      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 1)                   31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48572 (189.74 KB)\n",
      "Trainable params: 48421 (189.14 KB)\n",
      "Non-trainable params: 151 (608.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Preperation of layers\n",
    "conv1 = tf.keras.layers.Conv1D(\n",
    "    filters=60,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding=\"causal\",\n",
    "    activation=\"relu\",\n",
    "    input_shape=[None, len(df.columns) - 1],\n",
    ")\n",
    "\n",
    "# Bidirectional LSTM tf.keras.layers\n",
    "lstm1 = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(30, return_sequences=True, stateful=False)\n",
    ")\n",
    "lstm2 = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(30, return_sequences=False, stateful=True)\n",
    ")\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "# Model\n",
    "inputs = tf.keras.layers.Input(batch_size=32, shape=(None, len(df.columns) - 1))\n",
    "x = norm_layer(inputs)\n",
    "x = conv1(x)\n",
    "x = lstm1(x)\n",
    "\n",
    "x = lstm2(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Increased dropout\n",
    "x = tf.keras.layers.BatchNormalization()(x)  # Batch Normalization\n",
    "x = tf.keras.layers.Dense(\n",
    "    30,\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=regularizers.l2(0.01),\n",
    ")(\n",
    "    x\n",
    ")  # L2 regularization on Dense layer\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "# Optimizers and losses\n",
    "SGD_optimizer = tf.keras.optimizers.legacy.SGD(\n",
    "    learning_rate=0.001, momentum=0.9, nesterov=True\n",
    ")\n",
    "adam_opt = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "mse_loss = \"mse\"\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=mse_loss, metrics=[\"mae\", \"RootMeanSquaredError\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1537/1537 [==============================] - 69s 42ms/step - loss: 0.2411 - mae: 0.0827 - root_mean_squared_error: 0.2079 - val_loss: 0.0153 - val_mae: 0.0242 - val_root_mean_squared_error: 0.0323 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1537/1537 [==============================] - 62s 40ms/step - loss: 0.0046 - mae: 0.0303 - root_mean_squared_error: 0.0442 - val_loss: 5.2935e-04 - val_mae: 0.0133 - val_root_mean_squared_error: 0.0222 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1537/1537 [==============================] - 62s 41ms/step - loss: 0.0018 - mae: 0.0283 - root_mean_squared_error: 0.0422 - val_loss: 4.7535e-04 - val_mae: 0.0131 - val_root_mean_squared_error: 0.0218 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1537/1537 [==============================] - 63s 41ms/step - loss: 0.0017 - mae: 0.0274 - root_mean_squared_error: 0.0411 - val_loss: 4.6962e-04 - val_mae: 0.0131 - val_root_mean_squared_error: 0.0217 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1537/1537 [==============================] - 63s 41ms/step - loss: 0.0017 - mae: 0.0273 - root_mean_squared_error: 0.0411 - val_loss: 4.6772e-04 - val_mae: 0.0131 - val_root_mean_squared_error: 0.0216 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "1537/1537 [==============================] - 63s 41ms/step - loss: 0.0017 - mae: 0.0273 - root_mean_squared_error: 0.0410 - val_loss: 4.6589e-04 - val_mae: 0.0131 - val_root_mean_squared_error: 0.0216 - lr: 2.5000e-04\n",
      "Epoch 7/50\n",
      "1537/1537 [==============================] - 62s 40ms/step - loss: 0.0017 - mae: 0.0273 - root_mean_squared_error: 0.0410 - val_loss: 4.6680e-04 - val_mae: 0.0131 - val_root_mean_squared_error: 0.0216 - lr: 1.2500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30dd70dd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, monitor=\"val_mae\", restore_best_weights=True\n",
    ")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\"TB_regression_logs\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_mae\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=1,  # Number of epochs with no improvement\n",
    "    min_lr=0.0001,  # Minimum learning rate\n",
    ")\n",
    "model.fit(\n",
    "    train_df_stateful,\n",
    "    validation_data=valid_df_stateful,\n",
    "    callbacks=[early_stopping, tensorboard, reduce_lr],\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_regression_model.keras\")  # Save regression the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch X shape: (32, 48, 15)\n",
      "Train batch y shape: (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:42:02.652569: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2023-11-20 22:42:02.652591: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-20 22:42:02.652599: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-20 22:42:02.652627: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-20 22:42:02.652648: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BTC:USDT_target\n",
       "0.0    35421\n",
       "2.0     8564\n",
       "1.0     7320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = \"BTC/USDT\"\n",
    "feature_lags = []\n",
    "valid_split = 2000\n",
    "train_length = 48\n",
    "batch_size = 32\n",
    "seed = 44\n",
    "\n",
    "df = get_features_and_target(\n",
    "    symbol,\n",
    "    days_to_forecast=1,\n",
    "    feature_lags=feature_lags,\n",
    "    model_type=\"class\",\n",
    ")\n",
    "\n",
    "symbol = symbol.replace(\"/\", \":\")\n",
    "\n",
    "X = df.drop(columns=f\"{symbol}_target\")\n",
    "\n",
    "y = df[f\"{symbol}_target\"].copy()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = (\n",
    "    X[:-valid_split],\n",
    "    X[-valid_split:],\n",
    "    y[:-valid_split],\n",
    "    y[-valid_split:],\n",
    ")\n",
    "\n",
    "train_df = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_train,\n",
    "    targets=y_train[train_length:],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shaffles the sequences, not within sequences\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "valid_df = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    X_valid,\n",
    "    targets=y_valid[train_length:],\n",
    "    sequence_length=train_length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "for X_batch, y_batch in train_df.take(1):\n",
    "    print(\"Train batch X shape:\", X_batch.shape)\n",
    "    print(\"Train batch y shape:\", y_batch.shape)  # starts from y.iloc[48]\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:42:47.162845: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 15)]        0         \n",
      "                                                                 \n",
      " normalization (Normalizati  (None, None, 15)          31        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 60)          2760      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 60)          21840     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 60)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1830      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 93        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26554 (103.73 KB)\n",
      "Trainable params: 26523 (103.61 KB)\n",
      "Non-trainable params: 31 (128.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Preperation of layers\n",
    "conv1 = tf.keras.layers.Conv1D(\n",
    "    filters=60,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding=\"causal\",\n",
    "    activation=\"relu\",\n",
    "    input_shape=[None, len(df.columns) - 1],\n",
    ")\n",
    "\n",
    "# Bidirectional LSTM tf.keras.layers\n",
    "lstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(30, return_sequences=True))\n",
    "# lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(30, return_sequences=False))\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "# Model\n",
    "inputs = tf.keras.layers.Input(shape=(None, len(df.columns) - 1))\n",
    "x = norm_layer(inputs)\n",
    "x = conv1(x)\n",
    "x = lstm1(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "# x = lstm2(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "# Optimizers and loss\n",
    "SGD_optimizer = tf.keras.optimizers.legacy.SGD(\n",
    "    learning_rate=0.001, momentum=0.9, nesterov=True\n",
    ")\n",
    "adam_opt = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "\n",
    "categorical_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam_opt,\n",
    "    loss=categorical_loss,\n",
    "    metrics=[\"accuracy\", Precision(), Recall()],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m tensorboard \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(\u001b[39m\"\u001b[39m\u001b[39mTB_classification_logs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m reduce_lr \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,  \u001b[39m# Reduce learning rate by half\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     patience\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,  \u001b[39m# Number of epochs with no improvement\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     min_lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,  \u001b[39m# Minimum learning rate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     train_df,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalid_df,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping, tensorboard, reduce_lr],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weights,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivantrue/Desktop/Git/BTC_Algo/LSTM_algo.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/cd/xc6lw60d5711s9k_4hjp_dwc0000gn/T/__autograph_generated_fileamgn91dv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/confusion_metrics.py\", line 470, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 672, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     \"balanced\", classes=np.unique(y_train), y=y_train\n",
    "# )\n",
    "class_weights = {0: 0.48, 1: 2.26, 2: 1.95}\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, monitor=\"accuracy\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\"TB_classification_logs\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"accuracy\",\n",
    "    factor=0.5,  # Reduce learning rate by half\n",
    "    patience=1,  # Number of epochs with no improvement\n",
    "    min_lr=0.0001,  # Minimum learning rate\n",
    ")\n",
    "model.fit(\n",
    "    train_df,\n",
    "    validation_data=valid_df,\n",
    "    callbacks=[early_stopping, tensorboard, reduce_lr],\n",
    "    epochs=50,\n",
    "    class_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"last_classification_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
